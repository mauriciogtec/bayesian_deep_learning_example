{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate pseudo-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x22327de6e48>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUYCMoUcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x223279deb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 150\n",
    "np.random.seed(110104)\n",
    "x = 4.0 * (np.random.rand(n) - 0.5)\n",
    "xfeat = np.column_stack((x**i for i in range(1, 9))) # polynomials up to degree 8\n",
    "y = 2.0 - 0.2*x + 0.2*(2*x - 0.5)**2 + 0.2 * np.random.randn(n)\n",
    "y[2] = 4.0 # Artificial outliers\n",
    "y[3] = 7.0 # Artificial outliers\n",
    "y[10] = 5.0 # Artificial outliers\n",
    "y = y.reshape((n, 1))\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split data in test and train (we'll use only train for Bayesian evidence and test for cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_train = 75\n",
    "xfeat_train, y_train = xfeat[:n_train, :], y[:n_train, :]\n",
    "xfeat_test, y_test = xfeat[n_train:, :], y[n_train:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function to generate training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(xfeat, y, batch_size):\n",
    "    n_batches = xfeat.shape[0] // batch_size\n",
    "    for i in range(n_batches):\n",
    "        yield xfeat[i*batch_size:(i + 1)*batch_size,:], y[i*batch_size:(i + 1)*batch_size,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a very simple architecture with one hidden and a l2 regulariser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_input(dim_features, dim_output):\n",
    "    features = tf.placeholder(tf.float32, (None, dim_features))\n",
    "    target = tf.placeholder(tf.float32, (None, dim_output))\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    penalty = tf.placeholder(tf.float32)\n",
    "    precision = tf.placeholder(tf.float32)\n",
    "    return features, target, lr, penalty, precision\n",
    "    \n",
    "def hidden_layer(features, dim_features, dim_hidden):\n",
    "    # The weights are created as a vector so that taking hessian is easier\n",
    "    W = tf.Variable(tf.truncated_normal((dim_features*dim_hidden, ), stddev=1/np.sqrt(dim_features*dim_hidden)))\n",
    "    Wmat = tf.reshape(W, (dim_features, dim_hidden))\n",
    "    b = tf.Variable(tf.zeros(dim_hidden))\n",
    "    hidden = tf.nn.relu(tf.matmul(features, Wmat) + b)\n",
    "    return hidden, W, b\n",
    "\n",
    "def output_layer(hidden, dim_hidden, dim_output):\n",
    "    # The weights are created as a vector so that taking hessian is easier\n",
    "    W2 = tf.Variable(tf.truncated_normal((dim_hidden*dim_output, ), stddev=1/np.sqrt(dim_hidden*dim_output)))\n",
    "    W2mat = tf.reshape(W2, (dim_hidden, dim_output))\n",
    "    b2 = tf.Variable(tf.zeros(1))\n",
    "    output = tf.matmul(hidden, W2mat) + b2\n",
    "    return output, W2, b2\n",
    "\n",
    "def nn_loss(output, target, penalty, precision, W, W2):\n",
    "    errors = output - target\n",
    "    weights = tf.concat([W, W2], axis = 0)\n",
    "    loss = precision * tf.reduce_sum(tf.square(errors)) + penalty * tf.reduce_sum(tf.square(weights))\n",
    "    return loss, errors, weights\n",
    "\n",
    "def nn_optimization(loss, lr):\n",
    "    optim = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "    return optim\n",
    "    \n",
    "class neural_network():\n",
    "    def __init__(self, dim_features, dim_output, dim_hidden):\n",
    "        self.features, self.target, self.lr, self.penalty, self.precision = nn_input(dim_features, dim_output)\n",
    "        self.hidden, self.W, self.b = hidden_layer(self.features, dim_features, dim_hidden)\n",
    "        self.output, self.W2, self.b2 = output_layer(self.hidden, dim_hidden, dim_output)\n",
    "        self.loss, self.errors, self.weights = nn_loss(\n",
    "            self.output, self.target, self.penalty, self.precision, self.W, self.W2)\n",
    "        self.optim = nn_optimization(self.loss, self.lr)\n",
    "        self.hessian = tf.hessians(self.loss, [self.W, self.W2])[0] # Jesus! Hessians!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': <tf.Variable 'Variable:0' shape=(400,) dtype=float32_ref>,\n",
       " 'W2': <tf.Variable 'Variable_2:0' shape=(50,) dtype=float32_ref>,\n",
       " 'b': <tf.Variable 'Variable_1:0' shape=(50,) dtype=float32_ref>,\n",
       " 'b2': <tf.Variable 'Variable_3:0' shape=(1,) dtype=float32_ref>,\n",
       " 'errors': <tf.Tensor 'sub:0' shape=(?, 1) dtype=float32>,\n",
       " 'features': <tf.Tensor 'Placeholder:0' shape=(?, 8) dtype=float32>,\n",
       " 'hessian': <tf.Tensor 'TensorArrayStack/TensorArrayGatherV3:0' shape=(?, 400) dtype=float32>,\n",
       " 'hidden': <tf.Tensor 'Relu:0' shape=(?, 50) dtype=float32>,\n",
       " 'loss': <tf.Tensor 'add_2:0' shape=<unknown> dtype=float32>,\n",
       " 'lr': <tf.Tensor 'Placeholder_2:0' shape=<unknown> dtype=float32>,\n",
       " 'optim': <tf.Operation 'Adam' type=NoOp>,\n",
       " 'output': <tf.Tensor 'add_1:0' shape=(?, 1) dtype=float32>,\n",
       " 'penalty': <tf.Tensor 'Placeholder_3:0' shape=<unknown> dtype=float32>,\n",
       " 'precision': <tf.Tensor 'Placeholder_4:0' shape=<unknown> dtype=float32>,\n",
       " 'target': <tf.Tensor 'Placeholder_1:0' shape=(?, 1) dtype=float32>,\n",
       " 'weights': <tf.Tensor 'concat:0' shape=(450,) dtype=float32>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_features = xfeat_train.shape[1]\n",
    "dim_output = y_train.shape[1]\n",
    "dim_hidden = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "nn = neural_network(dim_features, dim_output, dim_hidden) # Takes some time to create just because there is a hessian!\n",
    "nn.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train and evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1 training loss 33981.3984\n",
      "epoch 50 training loss 712.9588\n",
      "epoch 100 training loss 677.3344\n",
      "epoch 150 training loss 720.5264\n",
      "epoch 200 training loss 699.6487\n",
      "epoch 250 training loss 714.2664\n",
      "epoch 300 training loss 699.9774\n",
      "epoch 350 training loss 696.7038\n",
      "epoch 400 training loss 688.4408\n",
      "epoch 450 training loss 690.7779\n",
      "epoch 500 training loss 690.3386\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAADSCAYAAACo2xNAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22329994b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 500\n",
    "print_every_epochs = 50\n",
    "batch_size = 10\n",
    "lr = 0.01\n",
    "penalty = 0.00000001\n",
    "precision = 25.0 # because we use noise with sigma = 0.2\n",
    "\n",
    "hyperparams= {nn.penalty: penalty, nn.precision: precision, nn.lr: lr}\n",
    "full_training_feed = {nn.features: xfeat_train, nn.target: y_train, **hyperparams}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Training\n",
    "    sess.run(tf.global_variables_initializer())\n",
    " \n",
    "    for epoch in range(epochs):\n",
    "        for batch_x, batch_y in get_batches(xfeat_train, y_train, batch_size):\n",
    "            batch_feed = {nn.features: batch_x, nn.target: batch_y, **hyperparams}\n",
    "            sess.run(nn.optim, batch_feed)\n",
    "        if epoch == 0 or (epoch  + 1) % print_every_epochs == 0:\n",
    "            loss = sess.run(nn.loss, full_training_feed)\n",
    "            print(\"epoch %2d training loss %4.4f\" % (epoch + 1, loss))\n",
    "\n",
    "    # Visualize fit, errors, and weights\n",
    "    errors, weights, output = sess.run([nn.errors, nn.weights, nn.output], full_training_feed)\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize = (12, 3))\n",
    "    ax0.scatter(xfeat_train[:,0], y_train)\n",
    "    ax0.scatter(xfeat_train[:,0], output)\n",
    "    ax0.set_title('Fitted vs Actual')\n",
    "    ax1.hist(errors)\n",
    "    ax1.set_title('distribution of errors')\n",
    "    ax2.hist(weights)\n",
    "    ax2.set_title(\"distribution of weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat this but with Bayesian evidence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_evidence(hessian, loss, penalty, precision, dim_hidden, n_obs, n_weights):\n",
    "    s, logdet = np.linalg.slogdet(hessian)\n",
    "    N, k = n_obs, n_weights\n",
    "    posterior_energy = -0.5 * loss + 0.5 * k * np.log(2*np.pi) - 0.5 * logdet\n",
    "    weights_energy = 0.5 * k * np.log(2 * np.pi / penalty) \n",
    "    model_energy = 0.5 * N * np.log(2 * np.pi / precision)\n",
    "    symmetry_factor = dim_hidden * np.log(2) + np.sum(np.log(np.arange(1, dim_hidden + 1)))\n",
    "    return symmetry_factor + posterior_energy - weights_energy - model_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_nn(\n",
    "        xfeat_train, y_train,\n",
    "        xfeat_test, y_test,\n",
    "        dim_hidden, \n",
    "        epochs,\n",
    "        batch_size,\n",
    "        lr,\n",
    "        penalty,\n",
    "        precision): \n",
    "    # Create network\n",
    "    dim_features = xfeat.shape[1]\n",
    "    dim_output = y.shape[1]\n",
    "    tf.reset_default_graph()\n",
    "    nn = neural_network(dim_features, dim_output, dim_hidden) # Takes some time to create just because there is a hessian!\n",
    "    # Feed dictionaries\n",
    "    hyperparams= {nn.penalty: penalty, nn.precision: precision, nn.lr: lr}\n",
    "    with tf.Session() as sess:\n",
    "        # Train\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(epochs):\n",
    "            for batch_x, batch_y in get_batches(xfeat_train, y_train, batch_size):\n",
    "                batch_feed = {nn.features: batch_x, nn.target: batch_y, nn.lr: lr, **hyperparams}\n",
    "                sess.run(nn.optim, batch_feed)\n",
    "        # Find Bayesian (Log)evidence\n",
    "        full_train_feed = {nn.features: xfeat_train, nn.target: y_train, **hyperparams}\n",
    "        full_test_feed = {nn.features: xfeat_test, nn.target: y_test, **hyperparams}\n",
    "        hessian, loss_train, output, weights = sess.run([nn.hessian, nn.loss, nn.output, nn.weights], full_train_feed)\n",
    "        evidence = log_evidence(hessian, loss_train, penalty, precision, dim_hidden, len(output), len(weights)) \n",
    "        loss_test = sess.run(nn.loss, full_test_feed)\n",
    "        return evidence, loss_train, loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and compare evidence for multiple precisions and noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden dim:    2  penalty:     0.00000001 evidence:   -410.43 train loss:    697.15 test loss:    136.79  attempt: 1\n",
      "hidden dim:    2  penalty:     0.00000001 evidence:   -410.16 train loss:    682.22 test loss:    141.81  attempt: 2\n",
      "hidden dim:    2  penalty:     0.00000001 evidence:   -485.08 train loss:    707.83 test loss:    334.14  attempt: 3\n",
      "hidden dim:    2  penalty:     0.00001000 evidence:   -414.18 train loss:    687.78 test loss:    141.21  attempt: 1\n",
      "hidden dim:    2  penalty:     0.00001000 evidence:   -417.13 train loss:    673.55 test loss:    161.87  attempt: 2\n",
      "hidden dim:    2  penalty:     0.00001000 evidence:   -376.52 train loss:    686.33 test loss:    136.02  attempt: 3\n",
      "hidden dim:    2  penalty:     0.00010000 evidence:   -925.07 train loss:   1729.56 test loss:    716.76  attempt: 1\n",
      "hidden dim:    2  penalty:     0.00010000 evidence:   -410.20 train loss:    722.38 test loss:    160.95  attempt: 2\n",
      "hidden dim:    2  penalty:     0.00010000 evidence:  -1526.38 train loss:   3130.97 test loss:   2026.06  attempt: 3\n",
      "hidden dim:    2  penalty:     0.00100000 evidence:   -349.43 train loss:    684.87 test loss:    136.48  attempt: 1\n",
      "hidden dim:    2  penalty:     0.00100000 evidence:   -386.89 train loss:    686.46 test loss:    156.80  attempt: 2\n",
      "hidden dim:    2  penalty:     0.00100000 evidence:   -353.62 train loss:    686.22 test loss:    141.28  attempt: 3\n",
      "hidden dim:    2  penalty:     0.01000000 evidence:   -355.71 train loss:    684.91 test loss:    154.45  attempt: 1\n",
      "hidden dim:    2  penalty:     0.01000000 evidence:   -353.95 train loss:    706.90 test loss:    184.03  attempt: 2\n",
      "hidden dim:    2  penalty:     0.01000000 evidence:   -476.05 train loss:    989.64 test loss:    760.51  attempt: 3\n",
      "hidden dim:    2  penalty:     0.10000000 evidence:   -551.24 train loss:   1116.97 test loss:    584.01  attempt: 1\n",
      "hidden dim:    2  penalty:     0.10000000 evidence:   -466.57 train loss:    989.26 test loss:    760.15  attempt: 2\n",
      "hidden dim:    2  penalty:     0.10000000 evidence:   -468.77 train loss:    977.17 test loss:    798.52  attempt: 3\n",
      "hidden dim:    2  penalty:     1.00000000 evidence:   -328.33 train loss:    690.50 test loss:    150.66  attempt: 1\n",
      "hidden dim:    2  penalty:     1.00000000 evidence:   -463.82 train loss:    984.07 test loss:    763.56  attempt: 2\n",
      "hidden dim:    2  penalty:     1.00000000 evidence:   -331.36 train loss:    710.23 test loss:    121.56  attempt: 3\n",
      "hidden dim:    2  penalty:    10.00000000 evidence:   -342.62 train loss:    747.39 test loss:    156.13  attempt: 1\n",
      "hidden dim:    2  penalty:    10.00000000 evidence:   -348.12 train loss:    759.06 test loss:    164.82  attempt: 2\n",
      "hidden dim:    2  penalty:    10.00000000 evidence:   -369.33 train loss:    806.05 test loss:    180.69  attempt: 3\n",
      "hidden dim:    2  penalty:   100.00000000 evidence:  -1506.22 train loss:   3108.87 test loss:   1879.87  attempt: 1\n",
      "hidden dim:    2  penalty:   100.00000000 evidence:   -548.58 train loss:   1177.86 test loss:    530.46  attempt: 2\n",
      "hidden dim:    2  penalty:   100.00000000 evidence:   -461.04 train loss:    994.89 test loss:    288.78  attempt: 3\n",
      "Average evidence for 2 hidden variables is -512.47\n",
      "hidden dim:    5  penalty:     0.00000001 evidence:   -571.14 train loss:    676.64 test loss:    194.72  attempt: 1\n",
      "hidden dim:    5  penalty:     0.00000001 evidence:   -523.65 train loss:    742.48 test loss:    161.41  attempt: 2\n",
      "hidden dim:    5  penalty:     0.00000001 evidence:   -622.62 train loss:    662.99 test loss:    244.57  attempt: 3\n",
      "hidden dim:    5  penalty:     0.00001000 evidence:   -454.13 train loss:    678.58 test loss:    138.91  attempt: 1\n",
      "hidden dim:    5  penalty:     0.00001000 evidence:   -452.27 train loss:    672.15 test loss:    137.96  attempt: 2\n",
      "hidden dim:    5  penalty:     0.00001000 evidence:   -458.01 train loss:    649.91 test loss:    215.10  attempt: 3\n",
      "hidden dim:    5  penalty:     0.00010000 evidence:   -426.56 train loss:    713.53 test loss:    146.79  attempt: 1\n",
      "hidden dim:    5  penalty:     0.00010000 evidence:   -428.65 train loss:    638.66 test loss:    221.40  attempt: 2\n",
      "hidden dim:    5  penalty:     0.00010000 evidence:   -435.54 train loss:    650.53 test loss:    210.84  attempt: 3\n",
      "hidden dim:    5  penalty:     0.00100000 evidence:   -426.49 train loss:    667.17 test loss:    248.55  attempt: 1\n",
      "hidden dim:    5  penalty:     0.00100000 evidence:   -377.23 train loss:    661.12 test loss:    201.18  attempt: 2\n",
      "hidden dim:    5  penalty:     0.00100000 evidence:   -428.10 train loss:    643.34 test loss:    248.62  attempt: 3\n",
      "hidden dim:    5  penalty:     0.01000000 evidence:   -394.60 train loss:    682.38 test loss:    251.65  attempt: 1\n",
      "hidden dim:    5  penalty:     0.01000000 evidence:   -407.69 train loss:    732.90 test loss:    142.98  attempt: 2\n",
      "hidden dim:    5  penalty:     0.01000000 evidence:   -373.23 train loss:    658.39 test loss:    190.94  attempt: 3\n",
      "hidden dim:    5  penalty:     0.10000000 evidence:   -367.16 train loss:    714.54 test loss:    373.73  attempt: 1\n",
      "hidden dim:    5  penalty:     0.10000000 evidence:   -346.99 train loss:    668.37 test loss:    157.80  attempt: 2\n",
      "hidden dim:    5  penalty:     0.10000000 evidence:   -358.06 train loss:    703.60 test loss:    131.42  attempt: 3\n",
      "hidden dim:    5  penalty:     1.00000000 evidence:   -324.68 train loss:    669.10 test loss:    169.08  attempt: 1\n",
      "hidden dim:    5  penalty:     1.00000000 evidence:   -349.76 train loss:    707.50 test loss:    212.75  attempt: 2\n",
      "hidden dim:    5  penalty:     1.00000000 evidence:   -336.08 train loss:    670.47 test loss:    136.77  attempt: 3\n",
      "hidden dim:    5  penalty:    10.00000000 evidence:   -349.05 train loss:    748.04 test loss:    193.74  attempt: 1\n",
      "hidden dim:    5  penalty:    10.00000000 evidence:   -403.11 train loss:    851.35 test loss:    443.48  attempt: 2\n",
      "hidden dim:    5  penalty:    10.00000000 evidence:   -382.39 train loss:    796.21 test loss:    234.22  attempt: 3\n",
      "hidden dim:    5  penalty:   100.00000000 evidence:   -457.71 train loss:   1003.13 test loss:    316.51  attempt: 1\n",
      "hidden dim:    5  penalty:   100.00000000 evidence:   -437.75 train loss:    966.25 test loss:    306.02  attempt: 2\n",
      "hidden dim:    5  penalty:   100.00000000 evidence:   -461.99 train loss:   1008.32 test loss:    307.77  attempt: 3\n",
      "Average evidence for 5 hidden variables is -420.54\n",
      "hidden dim:   10  penalty:     0.00000001 evidence:   -747.71 train loss:    667.07 test loss:    226.52  attempt: 1\n",
      "hidden dim:   10  penalty:     0.00000001 evidence:   -614.89 train loss:    688.17 test loss:    138.35  attempt: 2\n",
      "hidden dim:   10  penalty:     0.00000001 evidence:   -833.53 train loss:    664.23 test loss:    198.62  attempt: 3\n",
      "hidden dim:   10  penalty:     0.00001000 evidence:   -565.36 train loss:    659.41 test loss:    252.62  attempt: 1\n",
      "hidden dim:   10  penalty:     0.00001000 evidence:   -534.16 train loss:    697.19 test loss:    215.57  attempt: 2\n",
      "hidden dim:   10  penalty:     0.00001000 evidence:   -551.34 train loss:    675.55 test loss:    220.54  attempt: 3\n",
      "hidden dim:   10  penalty:     0.00010000 evidence:   -472.96 train loss:    651.63 test loss:    199.91  attempt: 1\n",
      "hidden dim:   10  penalty:     0.00010000 evidence:   -495.40 train loss:    686.86 test loss:    246.35  attempt: 2\n",
      "hidden dim:   10  penalty:     0.00010000 evidence:   -522.41 train loss:    692.92 test loss:    371.77  attempt: 3\n",
      "hidden dim:   10  penalty:     0.00100000 evidence:   -473.58 train loss:    679.15 test loss:    243.30  attempt: 1\n",
      "hidden dim:   10  penalty:     0.00100000 evidence:   -611.62 train loss:   1052.82 test loss:    504.89  attempt: 2\n",
      "hidden dim:   10  penalty:     0.00100000 evidence:   -458.09 train loss:    686.31 test loss:    243.93  attempt: 3\n",
      "hidden dim:   10  penalty:     0.01000000 evidence:   -392.87 train loss:    648.54 test loss:    186.13  attempt: 1\n",
      "hidden dim:   10  penalty:     0.01000000 evidence:   -409.63 train loss:    701.84 test loss:    164.77  attempt: 2\n",
      "hidden dim:   10  penalty:     0.01000000 evidence:   -402.22 train loss:    659.77 test loss:    259.71  attempt: 3\n",
      "hidden dim:   10  penalty:     0.10000000 evidence:   -360.79 train loss:    615.09 test loss:    228.24  attempt: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden dim:   10  penalty:     0.10000000 evidence:   -381.58 train loss:    691.64 test loss:    184.15  attempt: 2\n",
      "hidden dim:   10  penalty:     0.10000000 evidence:   -383.36 train loss:    709.92 test loss:    286.94  attempt: 3\n",
      "hidden dim:   10  penalty:     1.00000000 evidence:   -335.04 train loss:    662.64 test loss:    176.89  attempt: 1\n",
      "hidden dim:   10  penalty:     1.00000000 evidence:   -334.42 train loss:    644.99 test loss:    196.69  attempt: 2\n",
      "hidden dim:   10  penalty:     1.00000000 evidence:   -371.24 train loss:    747.28 test loss:    162.53  attempt: 3\n",
      "hidden dim:   10  penalty:    10.00000000 evidence:   -424.56 train loss:    898.76 test loss:    219.15  attempt: 1\n",
      "hidden dim:   10  penalty:    10.00000000 evidence:   -386.02 train loss:    819.19 test loss:    229.81  attempt: 2\n",
      "hidden dim:   10  penalty:    10.00000000 evidence:   -368.61 train loss:    790.39 test loss:    270.86  attempt: 3\n",
      "hidden dim:   10  penalty:   100.00000000 evidence:   -437.74 train loss:    981.29 test loss:    270.98  attempt: 1\n",
      "hidden dim:   10  penalty:   100.00000000 evidence:   -466.87 train loss:   1035.62 test loss:    272.11  attempt: 2\n",
      "hidden dim:   10  penalty:   100.00000000 evidence:   -974.16 train loss:   2054.22 test loss:    951.34  attempt: 3\n",
      "Average evidence for 10 hidden variables is -492.97\n",
      "hidden dim:   25  penalty:     0.00000001 evidence:  -1046.63 train loss:    636.41 test loss:    221.02  attempt: 1\n",
      "hidden dim:   25  penalty:     0.00000001 evidence:  -1170.75 train loss:    684.68 test loss:    233.03  attempt: 2\n",
      "hidden dim:   25  penalty:     0.00000001 evidence:   -941.86 train loss:    650.21 test loss:    211.76  attempt: 3\n",
      "hidden dim:   25  penalty:     0.00001000 evidence:   -720.12 train loss:    654.71 test loss:    252.59  attempt: 1\n",
      "hidden dim:   25  penalty:     0.00001000 evidence:   -666.54 train loss:    660.08 test loss:    241.07  attempt: 2\n",
      "hidden dim:   25  penalty:     0.00001000 evidence:   -723.21 train loss:    703.16 test loss:    224.59  attempt: 3\n",
      "hidden dim:   25  penalty:     0.00010000 evidence:   -627.44 train loss:    666.15 test loss:    272.98  attempt: 1\n",
      "hidden dim:   25  penalty:     0.00010000 evidence:   -633.23 train loss:    745.32 test loss:    233.71  attempt: 2\n",
      "hidden dim:   25  penalty:     0.00010000 evidence:   -594.50 train loss:    662.71 test loss:    210.75  attempt: 3\n",
      "hidden dim:   25  penalty:     0.00100000 evidence:   -508.60 train loss:    581.97 test loss:    173.21  attempt: 1\n",
      "hidden dim:   25  penalty:     0.00100000 evidence:   -527.05 train loss:    634.70 test loss:    262.10  attempt: 2\n",
      "hidden dim:   25  penalty:     0.00100000 evidence:   -529.98 train loss:    692.04 test loss:    234.03  attempt: 3\n",
      "hidden dim:   25  penalty:     0.01000000 evidence:   -456.48 train loss:    673.55 test loss:    287.84  attempt: 1\n",
      "hidden dim:   25  penalty:     0.01000000 evidence:   -475.60 train loss:    686.52 test loss:    229.87  attempt: 2\n",
      "hidden dim:   25  penalty:     0.01000000 evidence:   -433.87 train loss:    650.94 test loss:    216.15  attempt: 3\n",
      "hidden dim:   25  penalty:     0.10000000 evidence:   -388.53 train loss:    627.24 test loss:    234.92  attempt: 1\n",
      "hidden dim:   25  penalty:     0.10000000 evidence:   -386.11 train loss:    642.00 test loss:    233.92  attempt: 2\n",
      "hidden dim:   25  penalty:     0.10000000 evidence:   -391.60 train loss:    665.98 test loss:    189.90  attempt: 3\n",
      "hidden dim:   25  penalty:     1.00000000 evidence:   -354.09 train loss:    701.68 test loss:    199.17  attempt: 1\n",
      "hidden dim:   25  penalty:     1.00000000 evidence:   -367.89 train loss:    716.33 test loss:    237.04  attempt: 2\n",
      "hidden dim:   25  penalty:     1.00000000 evidence:   -354.47 train loss:    674.46 test loss:    242.29  attempt: 3\n",
      "hidden dim:   25  penalty:    10.00000000 evidence:   -349.85 train loss:    793.11 test loss:    160.39  attempt: 1\n",
      "hidden dim:   25  penalty:    10.00000000 evidence:   -309.94 train loss:    709.82 test loss:    176.34  attempt: 2\n",
      "hidden dim:   25  penalty:    10.00000000 evidence:   -378.15 train loss:    863.84 test loss:    207.01  attempt: 3\n",
      "hidden dim:   25  penalty:   100.00000000 evidence:   -410.26 train loss:   1019.32 test loss:    295.97  attempt: 1\n",
      "hidden dim:   25  penalty:   100.00000000 evidence:   -410.05 train loss:   1018.05 test loss:    269.98  attempt: 2\n",
      "hidden dim:   25  penalty:   100.00000000 evidence:   -397.64 train loss:    992.24 test loss:    267.99  attempt: 3\n",
      "Average evidence for 25 hidden variables is -539.05\n",
      "hidden dim:   50  penalty:     0.00000001 evidence:  -1704.39 train loss:    723.79 test loss:    392.26  attempt: 1\n",
      "hidden dim:   50  penalty:     0.00000001 evidence:  -1631.82 train loss:    643.83 test loss:    212.17  attempt: 2\n",
      "hidden dim:   50  penalty:     0.00000001 evidence:  -1784.17 train loss:    630.52 test loss:    290.16  attempt: 3\n",
      "hidden dim:   50  penalty:     0.00001000 evidence:   -856.48 train loss:    668.85 test loss:    261.11  attempt: 1\n",
      "hidden dim:   50  penalty:     0.00001000 evidence:   -883.52 train loss:    661.78 test loss:    248.13  attempt: 2\n",
      "hidden dim:   50  penalty:     0.00001000 evidence:   -852.67 train loss:    616.83 test loss:    230.32  attempt: 3\n",
      "hidden dim:   50  penalty:     0.00010000 evidence:   -692.59 train loss:    582.93 test loss:    199.78  attempt: 1\n",
      "hidden dim:   50  penalty:     0.00010000 evidence:   -701.51 train loss:    642.49 test loss:    222.48  attempt: 2\n",
      "hidden dim:   50  penalty:     0.00010000 evidence:   -721.21 train loss:    610.40 test loss:    234.45  attempt: 3\n",
      "hidden dim:   50  penalty:     0.00100000 evidence:   -588.53 train loss:    680.88 test loss:    230.86  attempt: 1\n",
      "hidden dim:   50  penalty:     0.00100000 evidence:   -568.83 train loss:    647.41 test loss:    219.04  attempt: 2\n",
      "hidden dim:   50  penalty:     0.00100000 evidence:   -574.65 train loss:    627.74 test loss:    205.14  attempt: 3\n",
      "hidden dim:   50  penalty:     0.01000000 evidence:   -474.52 train loss:    564.83 test loss:    201.53  attempt: 1\n",
      "hidden dim:   50  penalty:     0.01000000 evidence:   -492.43 train loss:    611.77 test loss:    186.48  attempt: 2\n",
      "hidden dim:   50  penalty:     0.01000000 evidence:   -469.92 train loss:    605.24 test loss:    217.39  attempt: 3\n",
      "hidden dim:   50  penalty:     0.10000000 evidence:   -385.34 train loss:    598.59 test loss:    242.20  attempt: 1\n",
      "hidden dim:   50  penalty:     0.10000000 evidence:   -374.11 train loss:    613.30 test loss:    230.21  attempt: 2\n",
      "hidden dim:   50  penalty:     0.10000000 evidence:   -382.44 train loss:    591.41 test loss:    240.34  attempt: 3\n",
      "hidden dim:   50  penalty:     1.00000000 evidence:   -315.67 train loss:    683.11 test loss:    249.87  attempt: 1\n",
      "hidden dim:   50  penalty:     1.00000000 evidence:   -290.06 train loss:    618.74 test loss:    246.10  attempt: 2\n",
      "hidden dim:   50  penalty:     1.00000000 evidence:   -299.56 train loss:    645.07 test loss:    227.52  attempt: 3\n",
      "hidden dim:   50  penalty:    10.00000000 evidence:   -235.21 train loss:    689.86 test loss:    179.09  attempt: 1\n",
      "hidden dim:   50  penalty:    10.00000000 evidence:   -264.82 train loss:    762.15 test loss:    184.26  attempt: 2\n",
      "hidden dim:   50  penalty:    10.00000000 evidence:   -242.39 train loss:    713.26 test loss:    182.53  attempt: 3\n",
      "hidden dim:   50  penalty:   100.00000000 evidence:   -300.05 train loss:    983.04 test loss:    260.38  attempt: 1\n",
      "hidden dim:   50  penalty:   100.00000000 evidence:   -298.74 train loss:    995.52 test loss:    297.28  attempt: 2\n",
      "hidden dim:   50  penalty:   100.00000000 evidence:   -298.94 train loss:    985.64 test loss:    261.31  attempt: 3\n",
      "Average evidence for 50 hidden variables is -617.95\n",
      "hidden dim:  100  penalty:     0.00000001 evidence:  -2105.12 train loss:    682.99 test loss:    316.33  attempt: 1\n",
      "hidden dim:  100  penalty:     0.00000001 evidence:  -2730.35 train loss:    609.18 test loss:    198.54  attempt: 2\n",
      "hidden dim:  100  penalty:     0.00000001 evidence:  -2404.82 train loss:    621.58 test loss:    229.51  attempt: 3\n",
      "hidden dim:  100  penalty:     0.00001000 evidence:  -1066.88 train loss:    603.96 test loss:    260.64  attempt: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden dim:  100  penalty:     0.00001000 evidence:  -1092.13 train loss:    614.42 test loss:    202.13  attempt: 2\n",
      "hidden dim:  100  penalty:     0.00001000 evidence:  -1152.09 train loss:    679.33 test loss:    247.12  attempt: 3\n",
      "hidden dim:  100  penalty:     0.00010000 evidence:   -853.79 train loss:    588.19 test loss:    219.19  attempt: 1\n",
      "hidden dim:  100  penalty:     0.00010000 evidence:   -857.81 train loss:    581.26 test loss:    213.99  attempt: 2\n",
      "hidden dim:  100  penalty:     0.00010000 evidence:   -840.04 train loss:    624.46 test loss:    238.29  attempt: 3\n",
      "hidden dim:  100  penalty:     0.00100000 evidence:   -667.34 train loss:    612.89 test loss:    221.42  attempt: 1\n",
      "hidden dim:  100  penalty:     0.00100000 evidence:   -652.77 train loss:    607.07 test loss:    214.61  attempt: 2\n",
      "hidden dim:  100  penalty:     0.00100000 evidence:   -689.79 train loss:    659.45 test loss:    216.04  attempt: 3\n",
      "hidden dim:  100  penalty:     0.01000000 evidence:   -535.66 train loss:    697.88 test loss:    258.19  attempt: 1\n",
      "hidden dim:  100  penalty:     0.01000000 evidence:   -505.73 train loss:    557.74 test loss:    192.49  attempt: 2\n",
      "hidden dim:  100  penalty:     0.01000000 evidence:   -524.70 train loss:    648.17 test loss:    305.40  attempt: 3\n",
      "hidden dim:  100  penalty:     0.10000000 evidence:   -338.55 train loss:    640.44 test loss:    244.98  attempt: 1\n",
      "hidden dim:  100  penalty:     0.10000000 evidence:   -351.31 train loss:    633.93 test loss:    243.35  attempt: 2\n",
      "hidden dim:  100  penalty:     0.10000000 evidence:   -333.06 train loss:    634.43 test loss:    189.38  attempt: 3\n",
      "hidden dim:  100  penalty:     1.00000000 evidence:   -199.59 train loss:    656.83 test loss:    275.86  attempt: 1\n",
      "hidden dim:  100  penalty:     1.00000000 evidence:   -184.86 train loss:    627.45 test loss:    188.35  attempt: 2\n",
      "hidden dim:  100  penalty:     1.00000000 evidence:   -176.75 train loss:    611.22 test loss:    197.93  attempt: 3\n",
      "hidden dim:  100  penalty:    10.00000000 evidence:   -109.45 train loss:    788.84 test loss:    200.00  attempt: 1\n",
      "hidden dim:  100  penalty:    10.00000000 evidence:   -114.29 train loss:    780.87 test loss:    197.02  attempt: 2\n",
      "hidden dim:  100  penalty:    10.00000000 evidence:    -76.23 train loss:    713.16 test loss:    170.74  attempt: 3\n",
      "hidden dim:  100  penalty:   100.00000000 evidence:   -110.92 train loss:   1062.23 test loss:    275.35  attempt: 1\n",
      "hidden dim:  100  penalty:   100.00000000 evidence:    -92.75 train loss:   1025.33 test loss:    262.04  attempt: 2\n",
      "hidden dim:  100  penalty:   100.00000000 evidence:    -79.82 train loss:    992.57 test loss:    266.74  attempt: 3\n",
      "Average evidence for 100 hidden variables is -698.02\n"
     ]
    }
   ],
   "source": [
    "# Optimization parameters\n",
    "epochs = 500\n",
    "batch_size = 10\n",
    "lr = 0.01\n",
    "attempts_per_setting = 3\n",
    "\n",
    "# Network parameters\n",
    "dim_hidden_list = [2, 5, 10, 25, 50, 100]\n",
    "penalty_list = [0.00000001, .00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "precision = 25.0\n",
    "\n",
    "evidence_trace = []\n",
    "generror_trace = []\n",
    "for dim_hidden in dim_hidden_list:   \n",
    "    dim_av_evidence = 0\n",
    "    for penalty in penalty_list:     \n",
    "        for attempt in range(attempts_per_setting):\n",
    "            bayes_train, loss_train, loss_test = test_nn(\n",
    "                xfeat_train, y_train, xfeat_test, y_test, dim_hidden, epochs, batch_size, lr, penalty, precision)\n",
    "            evidence_trace.append(bayes_train)\n",
    "            generror_trace.append(loss_test)\n",
    "            print_str = 'hidden dim: %4d  penalty: %14.8f evidence: %9.2f' + \\\n",
    "                ' train loss: %9.2f test loss: %9.2f  attempt: %d'\n",
    "            dim_av_evidence += bayes_train / len(penalty_list) / attempts_per_setting\n",
    "            print(print_str %(dim_hidden, penalty, bayes_train, loss_train, loss_test, attempt + 1))\n",
    "    print('Average evidence for %d hidden variables is %0.2f' % (dim_hidden, dim_av_evidence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
